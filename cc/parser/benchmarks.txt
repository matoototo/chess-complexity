~ => same model/values unless stated otherwise

1700 = 166k
0.0925 16x4x128
0.01, 0.9, weight_decay=1e-5

252k
0.0901 64x6x128
0.01, 0.9, weight_decay=1e-4

run1
558.5k
0.0885 64x6x128
0.01, 0.9, weight_decay=1e-4

run2
fix eval/train modes (cause test data leakage)
0.01, 0.9, weight_decay=1e-4

run3
256 BS (others are 1024)
0.01, 0.9, weight_decay=1e-4

run4
observed train loss spikes
0.01, 0.9, weight_decay=1e-4

run5
fixed train loss spikes
0.01, 0.9, weight_decay=1e-4

run6
0.005, 0.9, weight_decay=1e-4

run7
static gamma and beta
0.01, 0.9, weight_decay=1e-4

run8
adam 64x6x128
3e-4, drop to 1e-4 at 555k

run9
adam 128x10x128
3e-4, drop to 1e-4 at 851k

run10
current-eval plane
adam 64x6x128
3e-4

run11
enable biases in conv layers
adam 64x6x128
3e-4

run12 ~ run11
lazy planes conversion sanity check
adam 64x6x128
3e-4

run13 ~ run12
less logging and higher LR
adam 64x6x128
6e-4

run14
larger baseline
adam 128x10x128
6e-4

run15
larger baseline
adam 128x10x128
3e-4, 1e-4 at 1687571

run16
fixed last label bug
adam 64x6x128
3e-4, 1e-4 at 960k, 3e-5 at 1604k

run17 ~ run16
adamw wd = 1e-4
adam 64x6x128
3e-4

run18 ~ run17
trainable BN
adam 64x6x128
3e-4, 1e-4 at 2588934 (epoch #2)

run19 ~ run18
conv -> involution (naive)
adam 64x6x128
3e-4

run20 ~ run19
much better data shuffling
adam 64x6x128
3e-4, 1e-4 at 2030k

run21 ~ run20
larger network
adam 128x10x128
3e-4, 1e-4 at 2103k

run22 ~ run20
giving sgd another go
sgd 64x6x128
1e-2, 1e-3 at 448k

run23 ~ run20
trying mg optim
mg 64x6x128
3e-4, 3e-5 at 1763258, 3e-6 at 2608584

run24 ~ run23
larger network
mg 162x12x128
3e-4

run31
SGD with momentum
SGD 64x6x256 v2
0.2

run32 ~ run31
head v1
0.2, 0.02 @ 1115852, 0.002 @ 2611416

run33 ~ run31
4096 BS
0.2, 0.02 @ 1301159, 0.002 @ 1779863

run34 ~ run33
1e-4 lambda (L2)
0.2

run35 ~ run34
0.4 starting LR
0.4

run36 ~ run35
logging weight norms

run37 ~ run36
1e-5 lambda (L2)
