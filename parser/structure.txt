filter -> parser -> data -> (imported by) train

filter.cpp
    - filter games that do not have computer evaluations
    - filter useless headers

parser.py
    - convert each position to FEN, extract evaluations and header values

data.py
    - converts output from parser.py into a PyTorch dataset

try/todo
    - filter out low time control games, though probably after there is a baseline model in place
    - attackers plane?
    - history planes
    - tags prediction (scrape CT)
    - try different weight decay values (though cy uses same)
    - try adam
    - try LR warmup
    - add current eval to input?
    - larger parse chunks (500k~?)
    - shuffle train file order
    - determine cause for gradient norm increase in runs 5 and 6
    - try training with only the head
    - shuffle all positions in file (!)
        - would make files larger due to header duplication
    - disable weight decay
    - increase test and train size
    - load whole dataset into gpu memory
    - instead of parsing chunks of data at once, convert fens only when they need to be used (in the __getitem__)
    - ! drop or carry over small chunks (happens at end of file if there aren't enough)
    - either rename data files or add month information to path
