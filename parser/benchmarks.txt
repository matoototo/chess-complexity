~ => same model/values unless stated otherwise

1700 = 166k
0.0925 16x4x128
0.01, 0.9, weight_decay=1e-5

252k
0.0901 64x6x128
0.01, 0.9, weight_decay=1e-4

run1
558.5k
0.0885 64x6x128
0.01, 0.9, weight_decay=1e-4

run2
fix eval/train modes (cause test data leakage)
0.01, 0.9, weight_decay=1e-4

run3
256 BS (others are 1024)
0.01, 0.9, weight_decay=1e-4

run4
observed train loss spikes
0.01, 0.9, weight_decay=1e-4

run5
fixed train loss spikes
0.01, 0.9, weight_decay=1e-4

run6
0.005, 0.9, weight_decay=1e-4

run7
static gamma and beta
0.01, 0.9, weight_decay=1e-4

run8
adam 64x6x128
3e-4, drop to 1e-4 at 555k

run9
adam 128x10x128
3e-4, drop to 1e-4 at 851k

run10
current-eval plane
adam 64x6x128
3e-4

run11
enable biases in conv layers
adam 64x6x128
3e-4

run12 ~ run11
lazy planes conversion sanity check
adam 64x6x128
3e-4

run13 ~ run12
less logging and higher LR
adam 64x6x128
6e-4

run14
larger baseline
adam 128x10x128
6e-4

run15
larger baseline
adam 128x10x128
3e-4, 1e-4 at 1687571

run16
fixed last label bug
adam 64x6x128
3e-4, 1e-4 at 960k
